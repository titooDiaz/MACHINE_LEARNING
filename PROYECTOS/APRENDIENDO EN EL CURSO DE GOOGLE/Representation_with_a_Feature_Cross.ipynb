{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4r2z30vJSbA"
      },
      "source": [
        "# Colabs\n",
        "\n",
        "Machine Learning Crash Course uses Colaboratories (Colabs) for all programming exercises. Colab is Google's implementation of [Jupyter Notebook](https://jupyter.org/). For more information about Colabs and how to use them, go to [Welcome to Colaboratory](https://research.google.com/colaboratory)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5y5fY9Jy_x"
      },
      "source": [
        "# Representation with a Feature Cross\n",
        "\n",
        "In this exercise, you'll experiment with different ways to represent features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXWoPIezkzgI"
      },
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "After doing this Colab, you'll know how to:\n",
        "\n",
        "  * Use TensorFlow [preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) to represent features in different ways.\n",
        "  * Represent features as [bins](https://developers.google.com/machine-learning/glossary/#bucketing).\n",
        "  * Cross bins to create a [feature cross](https://developers.google.com/machine-learning/glossary/#feature_cross)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH_g3Hsfkzzb"
      },
      "source": [
        "## The Dataset\n",
        "  \n",
        "Like several of the previous Colabs, this exercise uses the [California Housing Dataset](https://developers.google.com/machine-learning/crash-course/california-housing-data-description)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iuw6-JOGf7I"
      },
      "source": [
        "## Call the import statements\n",
        "\n",
        "The following code imports the necessary code to run the code in the rest of this Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n9_cTveKmse"
      },
      "outputs": [],
      "source": [
        "#@title Load the imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# The following lines adjust the granularity of reporting.\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "print(\"Imported the modules.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY"
      },
      "source": [
        "## Load, scale, and shuffle the examples\n",
        "\n",
        "The following code cell loads the separate .csv files and creates the following two pandas DataFrames:\n",
        "\n",
        "* `train_df`, which contains the training set\n",
        "* `test_df`, which contains the test set\n",
        "\n",
        "The code cell then scales the `median_house_value` to a more human-friendly range and then shuffles the examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZlvdpyYKx7V"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")\n",
        "\n",
        "# Scale the labels\n",
        "scale_factor = 1000.0\n",
        "# Scale the training set's label.\n",
        "train_df[\"median_house_value\"] /= scale_factor\n",
        "\n",
        "# Scale the test set's label\n",
        "test_df[\"median_house_value\"] /= scale_factor\n",
        "\n",
        "# Shuffle the examples\n",
        "train_df = train_df.reindex(np.random.permutation(train_df.index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kir8UTUXSV8"
      },
      "source": [
        "## Represent latitude and longitude as floating-point values\n",
        "\n",
        "Previous Colabs trained on only a single feature or a single synthetic feature. By contrast, this exercise trains on two features using **Input layers**.\n",
        "\n",
        "A neighborhood's location is typically the most important feature in determining a house's value. The California Housing dataset provides two features, `latitude` and `longitude` that identify each neighborhood's location.\n",
        "\n",
        "The following code cell defines two [`tf.keras.Input`](https://keras.io/api/layers/core_layers/input/) layers, one to represent `latitude` and another one to represent `longitude`, both as floating-point values.\n",
        "\n",
        "This code cell specifies the features that you'll ultimately train the model on and how each of those features will be represented.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tmmZIDw4JEC"
      },
      "outputs": [],
      "source": [
        "# Keras Input tensors of float values.\n",
        "inputs = {\n",
        "    'latitude':\n",
        "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
        "                              name='latitude'),\n",
        "    'longitude':\n",
        "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
        "                              name='longitude')\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT"
      },
      "source": [
        "## Define functions that create and train a model, and a plotting function\n",
        "\n",
        "The following code defines three functions:\n",
        "\n",
        "  * `create_model`, which tells TensorFlow to build a linear regression model based on the inputs and outputs provided.\n",
        "  * `train_model`, which will ultimately train the model from training set examples.\n",
        "  * `plot_the_loss_curve`, which generates a loss curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pedD5GhlDC-y"
      },
      "outputs": [],
      "source": [
        "#@title Define functions to create and train a model, and a plotting function\n",
        "def create_model(my_inputs, my_outputs, my_learning_rate):\n",
        "\n",
        "  model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)\n",
        "\n",
        "  # Construct the layers into a model that TensorFlow can execute.\n",
        "  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(\n",
        "      learning_rate=my_learning_rate),\n",
        "      loss=\"mean_squared_error\",\n",
        "      metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(model, dataset, epochs, batch_size, label_name):\n",
        "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
        "\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=True)\n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "\n",
        "  # Isolate the mean absolute error for each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  rmse = hist[\"root_mean_squared_error\"]\n",
        "\n",
        "  return epochs, rmse\n",
        "\n",
        "\n",
        "def plot_the_loss_curve(epochs, rmse):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "  plt.plot(epochs, rmse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([rmse.min()*0.94, rmse.max()* 1.05])\n",
        "  plt.show()\n",
        "\n",
        "print(\"Defined the create_model, train_model, and plot_the_loss_curve functions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-IXYVfvM4gD"
      },
      "source": [
        "## Train the model with floating-point representations\n",
        "\n",
        "The following code cell calls the functions you just created to train, plot, and evaluate a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "nj3v5EKQFY8s"
      },
      "outputs": [],
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.05\n",
        "epochs = 30\n",
        "batch_size = 100\n",
        "label_name = 'median_house_value'\n",
        "\n",
        "# The two Input layers are concatenated so they can be passed as a single\n",
        "# tensor to a Dense layer.\n",
        "preprocessing_layer = tf.keras.layers.Concatenate()(inputs.values())\n",
        "\n",
        "dense_output = layers.Dense(units=1, name='dense_layer')(preprocessing_layer)\n",
        "\n",
        "outputs = {\n",
        "  'dense_output': dense_output\n",
        "}\n",
        "\n",
        "# Create and compile the model's topography.\n",
        "my_model = create_model(inputs, outputs, learning_rate)\n",
        "\n",
        "# To view a PNG of this model's layers, uncomment the call to\n",
        "# `tf.keras.utils.plot_model` below. After running this code cell, click\n",
        "# the file folder on the left, then the `my_model.png` file.\n",
        "# tf.keras.utils.plot_model(my_model, \"my_model.png\", show_shapes=True)\n",
        "\n",
        "# Train the model on the training set.\n",
        "epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n",
        "\n",
        "# Print out the model summary.\n",
        "my_model.summary(expand_nested=True)\n",
        "\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "\n",
        "print(\"\\n: Evaluate the new model against the test set:\")\n",
        "test_features = {name:np.array(value) for name, value in test_df.items()}\n",
        "test_label = np.array(test_features.pop(label_name))\n",
        "my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbyWNS6T2fIT"
      },
      "source": [
        "## Task 1: Why aren't floating-point values a good way to represent latitude and longitude?\n",
        "\n",
        "Are floating-point values a good way to represent `latitude` and `longitude`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VJLDCu5M2hXX"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view an answer to Task 1.\n",
        "\n",
        "# No. Representing latitude and longitude as\n",
        "# floating-point values does not have much\n",
        "# predictive power. For example, neighborhoods at\n",
        "# latitude 35 are not 36/35 more valuable\n",
        "# (or 35/36 less valuable) than houses at\n",
        "# latitude 36.\n",
        "\n",
        "# Representing `latitude` and `longitude` as\n",
        "# floating-point values provides almost no\n",
        "# predictive power. We're only using the raw values\n",
        "# to establish a baseline for future experiments\n",
        "# with better representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na8TPoPYx-0k"
      },
      "source": [
        "## Represent latitude and longitude in buckets\n",
        "\n",
        "The following code cell represents latitude and longitude in buckets (bins). Each bin represents all the neighborhoods within a single degree. For example,\n",
        "neighborhoods at latitude 35.4 and 35.8 are in the same bucket, but neighborhoods in latitude 35.4 and 36.2 are in different buckets.\n",
        "\n",
        "The model will learn a separate weight for each bucket. For example, the model will learn one weight for all the neighborhoods in the \"35\" bin, a different weight for neighborhoods in the \"36\" bin, and so on. This representation will create approximately 20 buckets:\n",
        "\n",
        "  * 10 buckets for `latitude`.\n",
        "  * 10 buckets for `longitude`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLTUFiaUyIpx"
      },
      "outputs": [],
      "source": [
        "resolution_in_degrees = 1.0\n",
        "\n",
        "# Create a list of numbers representing the bucket boundaries for latitude.\n",
        "latitude_boundaries = list(np.arange(int(min(train_df['latitude'])),\n",
        "                                     int(max(train_df['latitude'])),\n",
        "                                     resolution_in_degrees))\n",
        "print(\"latitude boundaries: \" + str(latitude_boundaries))\n",
        "\n",
        "# Create a Discretization layer to separate the latitude data into buckets.\n",
        "latitude = tf.keras.layers.Discretization(\n",
        "    bin_boundaries=latitude_boundaries,\n",
        "    name='discretization_latitude')(inputs.get('latitude'))\n",
        "\n",
        "# Number of categories is the length of latitude_boundaries plus one.\n",
        "latitude = tf.keras.layers.CategoryEncoding(\n",
        "    num_tokens=len(latitude_boundaries) + 1,\n",
        "    output_mode='one_hot',\n",
        "    name='category_encoding_latitude')(latitude)\n",
        "\n",
        "# Create a list of numbers representing the bucket boundaries for longitude.\n",
        "longitude_boundaries = list(np.arange(int(min(train_df['longitude'])),\n",
        "                                      int(max(train_df['longitude'])),\n",
        "                                      resolution_in_degrees))\n",
        "\n",
        "print(\"longitude boundaries: \" + str(longitude_boundaries))\n",
        "\n",
        "# Create a Discretization layer to separate the longitude data into buckets.\n",
        "longitude = tf.keras.layers.Discretization(\n",
        "    bin_boundaries=longitude_boundaries,\n",
        "    name='discretization_longitude')(inputs.get('longitude'))\n",
        "\n",
        "# Number of categories is the length of longitude_boundaries plus one.\n",
        "longitude = tf.keras.layers.CategoryEncoding(\n",
        "    num_tokens=len(longitude_boundaries) + 1,\n",
        "    output_mode='one_hot',\n",
        "    name='category_encoding_longitude')(longitude)\n",
        "\n",
        "# Concatenate latitude and longitude into a single tensor as input for the Dense layer.\n",
        "concatenate_layer = tf.keras.layers.Concatenate()([latitude, longitude])\n",
        "\n",
        "dense_output = layers.Dense(units=1, name='dense_layer')(concatenate_layer)\n",
        "\n",
        "# Define an output dictionary we'll send to the model constructor.\n",
        "outputs = {\n",
        "  'dense_output': dense_output\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZsFzoPQ4pFm"
      },
      "source": [
        "## Train the model with bucket representations\n",
        "\n",
        "Run the following code cell to train the model with bucket representations rather than floating-point representations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnDrghxBzLvD"
      },
      "outputs": [],
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.04\n",
        "epochs = 35\n",
        "\n",
        "# Build the model.\n",
        "my_model = create_model(inputs, outputs, learning_rate)\n",
        "\n",
        "# Train the model on the training set.\n",
        "epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n",
        "\n",
        "# Print out the model summary.\n",
        "my_model.summary(expand_nested=True)\n",
        "\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "\n",
        "print(\"\\n: Evaluate the new model against the test set:\")\n",
        "my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb-bIKsN5M48"
      },
      "source": [
        "## Task 2: Did buckets outperform floating-point representations?\n",
        "\n",
        "Compare the model's `root_mean_squared_error` values for the two representations (floating-point vs. buckets)?  Which model produced lower losses?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6sUlX1335UCb"
      },
      "outputs": [],
      "source": [
        "#@title Double-click for an answer to Task 2.\n",
        "\n",
        "# Bucket representation outperformed\n",
        "# floating-point representations.\n",
        "# However, you can still do far better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab6-bhUvxbTL"
      },
      "source": [
        "## Task 3: What is a better way to represent location?\n",
        "\n",
        "Buckets are a big improvement over floating-point values. Can you identify an even better way to identify location with `latitude` and `longitude`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5no5X0OFCwf7"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view an answer to Task 3.\n",
        "\n",
        "# Representing location as a feature cross should\n",
        "# produce better results.\n",
        "\n",
        "# In Task 2, you represented latitude in\n",
        "# one-dimensional buckets and longitude in\n",
        "# another series of one-dimensional buckets.\n",
        "# Real-world locations, however, exist in\n",
        "# two dimensions. Therefore, you should\n",
        "# represent location as a two-dimensional feature\n",
        "# cross. That is, you'll cross the 10 or so latitude\n",
        "# buckets with the 10 or so longitude buckets to\n",
        "# create a grid of 100 cells.\n",
        "\n",
        "# The model will learn separate weights for each\n",
        "# of the cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1ulCDiyGB6g"
      },
      "source": [
        "## Represent location as a feature cross\n",
        "\n",
        "The following code cell represents location as a feature cross. That is, the following code cell first creates buckets and then crosses the latitude and longitude features using a `HashedCrossing` layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "HunsuEzqn21s"
      },
      "outputs": [],
      "source": [
        "resolution_in_degrees = 1.0\n",
        "\n",
        "# Create a list of numbers representing the bucket boundaries for latitude.\n",
        "latitude_boundaries = list(np.arange(int(min(train_df['latitude'])),\n",
        "                                     int(max(train_df['latitude'])),\n",
        "                                     resolution_in_degrees))\n",
        "\n",
        "# Create a Discretization layer to separate the latitude data into buckets.\n",
        "latitude = tf.keras.layers.Discretization(\n",
        "    bin_boundaries=latitude_boundaries,\n",
        "    name='discretization_latitude')(inputs.get('latitude'))\n",
        "\n",
        "# Create a list of numbers representing the bucket boundaries for longitude.\n",
        "longitude_boundaries = list(np.arange(int(min(train_df['longitude'])),\n",
        "                                      int(max(train_df['longitude'])),\n",
        "                                      resolution_in_degrees))\n",
        "\n",
        "# Create a Discretization layer to separate the longitude data into buckets.\n",
        "longitude = tf.keras.layers.Discretization(\n",
        "    bin_boundaries=longitude_boundaries,\n",
        "    name='discretization_longitude')(inputs.get('longitude'))\n",
        "\n",
        "# Cross the latitude and longitude features into a single one-hot vector.\n",
        "feature_cross = tf.keras.layers.HashedCrossing(\n",
        "    num_bins=len(latitude_boundaries) * len(longitude_boundaries),\n",
        "    output_mode='one_hot',\n",
        "    name='cross_latitude_longitude')([latitude, longitude])\n",
        "\n",
        "dense_output = layers.Dense(units=1, name='dense_layer')(feature_cross)\n",
        "\n",
        "# Define an output dictionary we'll send to the model constructor.\n",
        "outputs = {\n",
        "  'dense_output': dense_output\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akRgNnnH3VXJ"
      },
      "source": [
        "Invoke the following code cell to test your solution for Task 3. Please ignore the warning messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn2PRDBEr5ni"
      },
      "outputs": [],
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.04\n",
        "epochs = 35\n",
        "\n",
        "# Build the model, this time passing in the feature_cross_feature_layer:\n",
        "my_model = create_model(inputs, outputs, learning_rate)\n",
        "\n",
        "# Train the model on the training set.\n",
        "epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n",
        "\n",
        "# Print out the model summary.\n",
        "my_model.summary(expand_nested=True)\n",
        "\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "\n",
        "print(\"\\n: Evaluate the new model against the test set:\")\n",
        "my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCT-l1GaWNQE"
      },
      "source": [
        "## Task 4: Did the feature cross outperform buckets?\n",
        "\n",
        "Compare the model's `root_mean_squared_error` values for the two representations (buckets vs. feature cross)?  Which model produced\n",
        "lower losses?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HUzdWDcs5rCi"
      },
      "outputs": [],
      "source": [
        "#@title Double-click for an answer to this question.\n",
        "\n",
        "# Yes, representing these features as a feature\n",
        "# cross produced much lower loss values than\n",
        "# representing these features as buckets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Iw3ljfXqSQ"
      },
      "source": [
        "## Task 5: Adjust the resolution of the feature cross\n",
        "\n",
        "Return to the code cell in the \"Represent location as a feature cross\" section. Notice that `resolution_in_degrees` is set to 1.0. Therefore, each cell represents an area of 1.0 degree of latitude by 1.0 degree of longitude, which corresponds to a cell of 110 km by 90 km.  This resolution defines a rather large neighborhood.\n",
        "\n",
        "Experiment with `resolution_in_degrees` to answer the following questions:\n",
        "\n",
        "  1. What value of `resolution_in_degrees` produces the best results (lowest loss value)?\n",
        "  2. Why does loss increase when the value of `resolution_in_degrees` drops below a certain value?\n",
        "\n",
        "Finally, answer the following question:\n",
        "\n",
        "  3. What feature (that does not exist in the California Housing Dataset) would\n",
        "     be a better proxy for location than latitude X longitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "71WWwlhx4h0X"
      },
      "outputs": [],
      "source": [
        "#@title Double-click for possible answers to Task 5.\n",
        "\n",
        "#1. A resolution of ~0.4 degree provides the best\n",
        "#   results.\n",
        "\n",
        "#2. Below ~0.4 degree, loss increases because the\n",
        "#   dataset does not contain enough examples in\n",
        "#   each cell to accurately predict prices for\n",
        "#   those cells.\n",
        "\n",
        "#3. Postal code would be a far better feature\n",
        "#   than latitude X longitude, assuming that\n",
        "#   the dataset contained sufficient examples\n",
        "#   in each postal code."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Representation with a Feature Cross.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
