{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4r2z30vJSbA"
      },
      "source": [
        "# Colabs\n",
        "\n",
        "Machine Learning Crash Course uses Colaboratories (Colabs) for all programming exercises. Colab is Google's implementation of [Jupyter Notebook](https://jupyter.org/). For more information about Colabs and how to use them, go to [Welcome to Colaboratory](https://research.google.com/colaboratory)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5y5fY9Jy_x"
      },
      "source": [
        "# Binary Classification\n",
        "\n",
        "So far, you've only created regression models. That is, you created models that produced floating-point predictions, such as, \"houses in this neighborhood costs N thousand dollars.\" In this Colab, you'll create and evaluate a binary [classification model](https://developers.google.com/machine-learning/glossary/#classification_model).  That is, you'll create a model that answers a binary question. In this exercise, the binary question will be, \"Are houses in this neighborhood above a certain price?\"\n",
        "\n",
        "\n",
        "\n",
        "# Clasificación Binaria\n",
        "\n",
        "Hasta ahora, solo has creado modelos de regresión. Es decir, has creado modelos que generan predicciones de punto flotante, como \"las casas en este vecindario cuestan N mil dólares\". En este Colab, crearás y evaluarás un [modelo de clasificación binaria](https://developers.google.com/machine-learning/glossary/#classification_model). Es decir, crearás un modelo que responde a una pregunta binaria. En este ejercicio, la pregunta binaria será: \"¿Las casas en este vecindario están por encima de cierto precio?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuw8rRl9lNuL"
      },
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "After doing this Colab, you'll know how to:\n",
        "\n",
        "  * Convert a regression question into a classification question.\n",
        "  * Modify the classification threshold and determine how that modification influences the model.\n",
        "  * Experiment with different classification metrics to determine your model's effectiveness.\n",
        "\n",
        "## Objetivos de Aprendizaje:\n",
        "\n",
        "Después de realizar este Colab, sabrás cómo:\n",
        "\n",
        "  * Convertir una pregunta de regresión en una pregunta de clasificación.\n",
        "  * Modificar el umbral de clasificación y determinar cómo esa modificación influye en el modelo.\n",
        "  * Experimentar con diferentes métricas de clasificación para determinar la efectividad de tu modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44OdC-OglN9D"
      },
      "source": [
        "## The Dataset\n",
        "  \n",
        "Like several of the previous Colabs, this Colab uses the [California Housing Dataset](https://developers.google.com/machine-learning/crash-course/california-housing-data-description).\n",
        "\n",
        "## El Conjunto de Datos\n",
        "\n",
        "Al igual que en varios de los Colabs anteriores, este Colab utiliza el [Conjunto de Datos de Viviendas de California](https://developers.google.com/machine-learning/crash-course/california-housing-data-description)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iuw6-JOGf7I"
      },
      "source": [
        "## Call the import statements\n",
        "\n",
        "The following code imports the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "9n9_cTveKmse"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\PERSONAL\\AppData\\Local\\Temp\\ipykernel_12208\\2260317277.py:4: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\PERSONAL\\OneDrive\\Documentos\\PROGRAMACION GITHUB\\MACHINE LEARNING\\env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "Ran the import statements.\n"
          ]
        }
      ],
      "source": [
        "#@title Load the imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# The following lines adjust the granularity of reporting.\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "# tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "print(\"Ran the import statements.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY"
      },
      "source": [
        "## Load the datasets from the internet\n",
        "\n",
        "The following code cell loads the separate .csv files and creates the following two pandas DataFrames:\n",
        "\n",
        "* `train_df`, which contains the training set\n",
        "* `test_df`, which contains the test set\n",
        "\n",
        "## Cargar los conjuntos de datos desde internet\n",
        "\n",
        "La siguiente celda de código carga los archivos .csv por separado y crea los siguientes dos DataFrames de pandas:\n",
        "\n",
        "* `train_df`, que contiene el conjunto de entrenamiento\n",
        "* `test_df`, que contiene el conjunto de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JZlvdpyYKx7V"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")\n",
        "train_df = train_df.reindex(np.random.permutation(train_df.index)) # shuffle the training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_vuAQq0Cvrp"
      },
      "source": [
        "Unlike some of the previous Colabs, the preceding code cell did not scale the label (`median_house_value`).  The following section (\"Normalize values\") provides an alternative approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G6y-XcEmk6r"
      },
      "source": [
        "## Normalize values\n",
        "\n",
        "When creating a model with multiple features, the values of each feature should cover roughly the same range.  For example, if one feature's range spans 500 to 100,000 and another feature's range spans 2 to 12, then the model will be difficult or impossible to train. Therefore, you should\n",
        "[normalize](https://developers.google.com/machine-learning/glossary/#normalization) features in a multi-feature model.\n",
        "\n",
        "The following code cell normalizes datasets by converting each raw value (including the label) to its Z-score. A **Z-score** is the number of standard deviations from the mean for a particular raw value. For example, consider a feature having the following characteristics:\n",
        "\n",
        "  * The mean is 60.\n",
        "  * The standard deviation is 10.\n",
        "\n",
        "The raw value 75 would have a Z-score of +1.5:\n",
        "\n",
        "```\n",
        "  Z-score = (75 - 60) / 10 = +1.5\n",
        "```\n",
        "\n",
        "The raw value 38 would have a Z-score of -2.2:\n",
        "\n",
        "```\n",
        "  Z-score = (38 - 60) / 10 = -2.2\n",
        "```\n",
        "\n",
        "\n",
        "## Normalizar valores\n",
        "\n",
        "Cuando se crea un modelo con múltiples características, los valores de cada característica deben abarcar aproximadamente el mismo rango. Por ejemplo, si el rango de una característica va desde 500 hasta 100,000 y el rango de otra característica va desde 2 hasta 12, entonces será difícil o incluso imposible entrenar el modelo. Por lo tanto, se debe realizar una [normalización](https://developers.google.com/machine-learning/glossary/#normalization) de las características en un modelo multi-característica.\n",
        "\n",
        "La siguiente celda de código normaliza los conjuntos de datos convirtiendo cada valor crudo (incluyendo la etiqueta) a su puntuación Z. Una **puntuación Z** es el número de desviaciones estándar respecto a la media para un valor crudo específico. Por ejemplo, considera una característica con las siguientes características:\n",
        "\n",
        "  * La media es 60.\n",
        "  * La desviación estándar es 10.\n",
        "\n",
        "El valor crudo 75 tendría una puntuación Z de +1.5:\n",
        "\n",
        "```\n",
        "  Puntuación Z = (75 - 60) / 10 = +1.5\n",
        "```\n",
        "\n",
        "El valor crudo 38 tendría una puntuación Z de -2.2:\n",
        "\n",
        "```\n",
        "  Puntuación Z = (38 - 60) / 10 = -2.2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n7nuAHoZIgVI"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11027</th>\n",
              "      <td>-0.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.9</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>815</th>\n",
              "      <td>1.2</td>\n",
              "      <td>-1.2</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>-0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9259</th>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7662</th>\n",
              "      <td>0.6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>0.3</td>\n",
              "      <td>-1.1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6532</th>\n",
              "      <td>0.6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>0.6</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.3</td>\n",
              "      <td>-0.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "11027       -0.7       0.4                 1.9         -0.4            -0.6   \n",
              "815          1.2      -1.2                -0.3         -0.2            -0.4   \n",
              "9259         0.2      -0.6                -0.7          1.0             0.8   \n",
              "7662         0.6      -0.8                 0.3         -1.1            -1.0   \n",
              "6532         0.6      -0.8                 0.6         -0.5            -0.5   \n",
              "\n",
              "       population  households  median_income  median_house_value  \n",
              "11027        -0.7        -0.7           -0.3                 0.2  \n",
              "815          -0.7        -0.4           -0.4                -0.3  \n",
              "9259          1.2         1.0            0.1                 0.1  \n",
              "7662         -1.1        -1.0           -0.3                 0.4  \n",
              "6532         -0.0        -0.5            0.3                -0.3  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the Z-scores of each column in the training set and\n",
        "# write those Z-scores into a new pandas DataFrame named train_df_norm.\n",
        "# Calcular las puntuaciones Z de cada columna en el conjunto de entrenamiento y\n",
        "# escribir esas puntuaciones Z en un nuevo DataFrame de pandas llamado train_df_norm.\n",
        "train_df_mean = train_df.mean()\n",
        "train_df_std = train_df.std()\n",
        "train_df_norm = (train_df - train_df_mean)/train_df_std\n",
        "\n",
        "# Examine some of the values of the normalized training set. Notice that most\n",
        "# Z-scores fall between -2 and +2.\n",
        "# Examinar algunos de los valores del conjunto de entrenamiento normalizado. Observa que la mayoría\n",
        "# de las puntuaciones Z caen entre -2 y +2.\n",
        "train_df_norm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QoW-59jVFF2I"
      },
      "outputs": [],
      "source": [
        "# Calcular las puntuaciones Z de cada columna en el conjunto de prueba y\n",
        "# escribir esas puntuaciones Z en un nuevo DataFrame de pandas llamado test_df_norm.\n",
        "test_df_norm = (test_df - train_df_mean) / train_df_std\n",
        "\n",
        "# Ten en cuenta que transformamos los datos de prueba con los valores calculados a partir del conjunto de entrenamiento,\n",
        "# ya que siempre debes transformar tus conjuntos de datos con exactamente los mismos valores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-swmXtWnZGis"
      },
      "source": [
        "## Task 1: Create a binary label\n",
        "\n",
        "In classification problems, the label for every example must be either 0 or 1. Unfortunately, the natural label in the California Housing Dataset, `median_house_value`, contains floating-point values like 80,100 or 85,700 rather than 0s and 1s, while the normalized version of `median_house_values` contains floating-point values primarily between -3 and +3.\n",
        "\n",
        "Your task is to create a new column named `median_house_value_is_high` in both the training set and the test set . If the `median_house_value` is higher than a certain arbitrary value (defined by `threshold`), then set `median_house_value_is_high` to 1. Otherwise, set `median_house_value_is_high` to 0.\n",
        "\n",
        "**Hint:** The cells in the `median_house_value_is_high` column must each hold `1` and `0`, not `True` and `False`. To convert `True` and `False` to  `1` and `0`, call the pandas DataFrame function `astype(float)`.\n",
        "\n",
        "\n",
        "## Tarea 1: Crear una etiqueta binaria\n",
        "\n",
        "En problemas de clasificación, la etiqueta para cada ejemplo debe ser 0 o 1. Desafortunadamente, la etiqueta natural en el conjunto de datos de viviendas de California, `median_house_value`, contiene valores de punto flotante como 80,100 o 85,700 en lugar de 0s y 1s, mientras que la versión normalizada de `median_house_values` contiene valores de punto flotante principalmente entre -3 y +3.\n",
        "\n",
        "Tu tarea es crear una nueva columna llamada `median_house_value_is_high` tanto en el conjunto de entrenamiento como en el conjunto de prueba. Si el valor de `median_house_value` es mayor que un valor arbitrario específico (definido por `threshold`), establece `median_house_value_is_high` en 1. De lo contrario, establece `median_house_value_is_high` en 0.\n",
        "\n",
        "**Pista:** Las celdas en la columna `median_house_value_is_high` deben contener cada una `1` y `0`, no `True` y `False`. Para convertir `True` y `False` a `1` y `0`, utiliza la función `astype(float)` del DataFrame de pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d4kWfWA8bhKW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11027   0.0\n",
              "815     0.0\n",
              "9259    0.0\n",
              "7662    0.0\n",
              "6532    0.0\n",
              "         ..\n",
              "342     0.0\n",
              "8321    0.0\n",
              "2182    0.0\n",
              "5238    0.0\n",
              "1848    0.0\n",
              "Name: median_house_value_is_high, Length: 8000, dtype: float64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Establecimos arbitrariamente el umbral en 265,000, que es\n",
        "# el percentil 75 para los valores medios de las casas. Cada vecindario\n",
        "# con un precio medio de casa superior a 265,000 será etiquetado como 1,\n",
        "# y todos los demás vecindarios serán etiquetados como 0.\n",
        "\n",
        "threshold = 265000 # This is the 75th percentile for median house values.\n",
        "train_df_norm[\"median_house_value_is_high\"] = (train_df[\"median_house_value\"] > threshold).astype(float) #neuva celda\n",
        "test_df_norm[\"median_house_value_is_high\"] = (train_df[\"median_house_value\"] > threshold).astype(float) #nueva celda\n",
        "\n",
        "# Print out a few example cells from the beginning and\n",
        "# middle of the training set, just to make sure that\n",
        "# your code created only 0s and 1s in the newly created\n",
        "# median_house_value_is_high column\n",
        "train_df_norm[\"median_house_value_is_high\"].head(8000)\n",
        "\n",
        "# Alternativamente, en lugar de elegir el umbral\n",
        "# basado en los valores de vivienda en bruto, puedes trabajar con puntuaciones Z.\n",
        "# Por ejemplo, la siguiente solución posible utiliza una puntuación Z\n",
        "# de +1.0 como el umbral, lo que significa que no más\n",
        "# del 16% de los valores en median_house_value_is_high\n",
        "# serán etiquetados como 1.\n",
        "\n",
        "# threshold_in_Z = 1.0\n",
        "# train_df_norm[\"median_house_value_is_high\"] = (train_df_norm[\"median_house_value\"] > threshold_in_Z).astype(float)\n",
        "# test_df_norm[\"median_house_value_is_high\"] = (test_df_norm[\"median_house_value\"] > threshold_in_Z).astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kir8UTUXSV8"
      },
      "source": [
        "## Represent features as input layers\n",
        "\n",
        "This code cell specifies the features, `median_income` and ` total_rooms`, that you'll ultimately train the model on. These [Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input) objects are instantiated as Keras tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tmmZIDw4JEC"
      },
      "outputs": [],
      "source": [
        "inputs = {\n",
        "# Features used to train the model on.\n",
        "  'median_income': tf.keras.Input(shape=(1,)),\n",
        "  'total_rooms': tf.keras.Input(shape=(1,))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT"
      },
      "source": [
        "## Define functions that build and train a model\n",
        "\n",
        "The following code cell defines two functions:\n",
        "\n",
        "  * `create_model(inputs, learning_rate, METRICS)`, which defines the model's\n",
        "    topography.\n",
        "  * `train_model(model, dataset, epochs, label_name, batch_size, shuffle)`, uses input features and labels to train the model.\n",
        "\n",
        "Prior exercises used [ReLU](https://developers.google.com/machine-learning/glossary#ReLU) as the [activation function](https://developers.google.com/machine-learning/glossary#activation-function). By contrast, this exercise uses [sigmoid](https://developers.google.com/machine-learning/glossary#sigmoid-function) as the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pedD5GhlDC-y"
      },
      "outputs": [],
      "source": [
        "#@title Define the functions that create and train a model.\n",
        "def create_model(my_inputs, my_learning_rate, METRICS):\n",
        "  # Use a Concatenate layer to concatenate the input layers into a single tensor.\n",
        "  # as input for the Dense layer. Ex: [input_1[0][0], input_2[0][0]]\n",
        "  concatenated_inputs = tf.keras.layers.Concatenate()(my_inputs.values())\n",
        "  dense = layers.Dense(units=1, name='dense_layer', activation=tf.sigmoid)\n",
        "  dense_output = dense(concatenated_inputs)\n",
        "  \"\"\"Create and compile a simple classification model.\"\"\"\n",
        "  my_outputs = {\n",
        "    'dense': dense_output,\n",
        "  }\n",
        "  model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)\n",
        "\n",
        "  # Call the compile method to construct the layers into a model that\n",
        "  # TensorFlow can execute.  Notice that we're using a different loss\n",
        "  # function for classification than for regression.\n",
        "  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=my_learning_rate),\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                metrics=METRICS)\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(model, dataset, epochs, label_name,\n",
        "                batch_size=None, shuffle=True):\n",
        "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
        "\n",
        "  # The x parameter of tf.keras.Model.fit can be a list of arrays, where\n",
        "  # each array contains the data for one feature.  Here, we're passing\n",
        "  # every column in the dataset. Note that the feature_layer will filter\n",
        "  # away most of those columns, leaving only the desired columns and their\n",
        "  # representations as features.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=shuffle)\n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "\n",
        "  # Isolate the classification metric for each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  return epochs, hist\n",
        "\n",
        "print(\"Defined the create_model and train_model functions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak_TMAzGOIFq"
      },
      "source": [
        "## Define a plotting function\n",
        "\n",
        "The following [matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) function plots one or more curves, showing how various classification metrics change with each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QF0BFRXTOeR3"
      },
      "outputs": [],
      "source": [
        "#@title Define the plotting function.\n",
        "def plot_curve(epochs, hist, list_of_metrics):\n",
        "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"\n",
        "  # list_of_metrics should be one of the names shown in:\n",
        "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Value\")\n",
        "\n",
        "  for m in list_of_metrics:\n",
        "    x = hist[m]\n",
        "    plt.plot(epochs[1:], x[1:], label=m)\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "print(\"Defined the plot_curve function.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-IXYVfvM4gD"
      },
      "source": [
        "## Invoke the creating, training, and plotting functions\n",
        "\n",
        "The following code cell calls specify the hyperparameters, and then invokes the\n",
        "functions to create and train the model, and then to plot the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "nj3v5EKQFY8s"
      },
      "outputs": [],
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "batch_size = 100\n",
        "label_name = \"median_house_value_is_high\"\n",
        "classification_threshold = 0.35\n",
        "\n",
        "# Establish the metrics the model will measure.\n",
        "METRICS = [\n",
        "           tf.keras.metrics.BinaryAccuracy(name='accuracy',\n",
        "                                           threshold=classification_threshold),\n",
        "          ]\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(inputs, learning_rate, METRICS)\n",
        "\n",
        "# To view a PNG of this model's layers, uncomment the call to\n",
        "# `tf.keras.utils.plot_model` below. After running this code cell, click\n",
        "# the file folder on the left, then the `my_classification_model.png` file.\n",
        "# tf.keras.utils.plot_model(my_model, \"my_classification_model.png\")\n",
        "\n",
        "# Train the model on the training set.\n",
        "epochs, hist = train_model(my_model, train_df_norm, epochs,\n",
        "                           label_name, batch_size)\n",
        "\n",
        "# Plot a graph of the metric(s) vs. epochs.\n",
        "list_of_metrics_to_plot = ['accuracy']\n",
        "\n",
        "plot_curve(epochs, hist, list_of_metrics_to_plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF64TpqkbOpn"
      },
      "source": [
        "Accuracy should gradually improve during training (until it can\n",
        "improve no more)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xNqWWos_zyk"
      },
      "source": [
        "## Evaluate the model against the test set\n",
        "\n",
        "At the end of model training, you ended up with a certain accuracy against the *training set*. Invoke the following code cell to determine your model's accuracy against the *test set*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJorkMlDmtHf"
      },
      "outputs": [],
      "source": [
        "features = {name:np.array(value) for name, value in test_df_norm.items()}\n",
        "label = np.array(features.pop(label_name))\n",
        "\n",
        "my_model.evaluate(x = features, y = label, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7cHkFXalXV5"
      },
      "source": [
        "## Task 2: How accurate is your model really?\n",
        "\n",
        "Is your model valuable?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUvCrQkulwjV"
      },
      "outputs": [],
      "source": [
        "#@title Double-click for a possible answer to Task 2.\n",
        "\n",
        "# A perfect model would make 100% accurate predictions.\n",
        "# Our model makes 80% accurate predictions. 80% sounds\n",
        "# good, but note that a model that always guesses\n",
        "# \"median_house_value_is_high is False\" would be 75%\n",
        "# accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8crSCCVf6gm"
      },
      "source": [
        "## Task 3: Add precision and recall as metrics\n",
        "\n",
        "Relying solely on accuracy, particularly for a class-imbalanced data set (like ours), can be a poor way to judge a classification model.  Modify the code in the following code cell to enable the model to measure not only accuracy but also precision and recall. We have\n",
        "added accuracy and precision; your task is to add recall. See the [TensorFlow Reference](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall) for details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-k1MD2XArmO"
      },
      "outputs": [],
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "batch_size = 100\n",
        "classification_threshold = 0.35\n",
        "label_name = \"median_house_value_is_high\"\n",
        "\n",
        "# Modify the following definition of METRICS to generate\n",
        "# not only accuracy and precision, but also recall:\n",
        "METRICS = [\n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy',\n",
        "                                      threshold=classification_threshold),\n",
        "      tf.keras.metrics.Precision(thresholds=classification_threshold,\n",
        "                                 name='precision'\n",
        "                                 ),\n",
        "      ?  # write code here\n",
        "]\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(inputs, learning_rate, METRICS)\n",
        "\n",
        "# Train the model on the training set.\n",
        "epochs, hist = train_model(my_model, train_df_norm, epochs,\n",
        "                           label_name, batch_size)\n",
        "\n",
        "# Plot metrics vs. epochs\n",
        "list_of_metrics_to_plot = ['accuracy', 'precision', 'recall']\n",
        "plot_curve(epochs, hist, list_of_metrics_to_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ax87gOyDBhAu"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view the solution for Task 3.\n",
        "\n",
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "batch_size = 100\n",
        "classification_threshold = 0.35\n",
        "label_name = \"median_house_value_is_high\"\n",
        "\n",
        "# Here is the updated definition of METRICS:\n",
        "METRICS = [\n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy',\n",
        "                                      threshold=classification_threshold),\n",
        "      tf.keras.metrics.Precision(thresholds=classification_threshold,\n",
        "                                 name='precision'\n",
        "                                 ),\n",
        "      tf.keras.metrics.Recall(thresholds=classification_threshold,\n",
        "                              name=\"recall\"),\n",
        "]\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(inputs, learning_rate, METRICS)\n",
        "\n",
        "# Train the model on the training set.\n",
        "epochs, hist = train_model(my_model, train_df_norm, epochs,\n",
        "                           label_name, batch_size)\n",
        "\n",
        "# Plot metrics vs. epochs\n",
        "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\"]\n",
        "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
        "\n",
        "\n",
        "# The new graphs suggest that precision and recall are\n",
        "# somewhat in conflict. That is, improvements to one of\n",
        "# those metrics may hurt the other metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAsB85iKSXLe"
      },
      "source": [
        "## Task 4: Experiment with the classification threshold (if time permits)\n",
        "\n",
        "Experiment with different values for `classification_threshold` in the code cell within \"Invoke the creating, training, and plotting functions.\"  What value of `classification_threshold` produces the highest accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FLPDYI7Sphnj"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view the solution for Task 4.\n",
        "\n",
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "batch_size = 100\n",
        "classification_threshold = 0.52\n",
        "label_name = \"median_house_value_is_high\"\n",
        "\n",
        "# Here is the updated definition of METRICS:\n",
        "METRICS = [\n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy',\n",
        "                                      threshold=classification_threshold),\n",
        "      tf.keras.metrics.Precision(thresholds=classification_threshold,\n",
        "                                 name='precision'\n",
        "                                 ),\n",
        "      tf.keras.metrics.Recall(thresholds=classification_threshold,\n",
        "                              name=\"recall\"),\n",
        "]\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(inputs, learning_rate, METRICS)\n",
        "\n",
        "# Train the model on the training set.\n",
        "epochs, hist = train_model(my_model, train_df_norm, epochs,\n",
        "                           label_name, batch_size)\n",
        "\n",
        "# Plot metrics vs. epochs\n",
        "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\"]\n",
        "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
        "\n",
        "# A `classification_threshold` of slightly over 0.5\n",
        "# appears to produce the highest accuracy (about 83%).\n",
        "# Raising the `classification_threshold` to 0.9 drops\n",
        "# accuracy by about 5%.  Lowering the\n",
        "# `classification_threshold` to 0.3 drops accuracy by\n",
        "# about 3%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBGRS0Ndduus"
      },
      "source": [
        "## Task 5: Summarize model performance (if time permits)\n",
        "\n",
        "If time permits, add one more metric that attempts to summarize the model's overall performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vwNE6syoFvWe"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view the solution for Task 5.\n",
        "\n",
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "batch_size = 100\n",
        "label_name = \"median_house_value_is_high\"\n",
        "\n",
        "# AUC is a reasonable \"summary\" metric for\n",
        "# classification models.\n",
        "# Here is the updated definition of METRICS to\n",
        "# measure AUC:\n",
        "METRICS = [\n",
        "      tf.keras.metrics.AUC(num_thresholds=100, name='auc'),\n",
        "]\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(inputs, learning_rate, METRICS)\n",
        "\n",
        "# Train the model on the training set.\n",
        "epochs, hist = train_model(my_model, train_df_norm, epochs,\n",
        "                           label_name, batch_size)\n",
        "\n",
        "# Plot metrics vs. epochs\n",
        "list_of_metrics_to_plot = ['auc']\n",
        "plot_curve(epochs, hist, list_of_metrics_to_plot)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Binary Classification.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
